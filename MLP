import numpy as np

class Perceptron:
    """A single neuron with the sigmoid activation funciton.

    Attributes:
        inputs: the number of inputs in the perceptron (not counting the bias)
        bias: The bias term. By default it is 1.0.
    """
    def __init__(self, inputs, bias = 1.0):
        """Return a new Perceptron object with the specified number of inputs 
        (+1 for ???)"""
        #initialize number data which are weights and bias term for this example
        self.weights = (np.random.rand(inputs+1)*2)-1
        self.bias = bias

    def run(self, x):
        """Run the perceptron. x is a python list with the input values.
        feeds an input array X into the perceptron to return the activation functions output
        """
        #weighed sum
        x_sum = np.dot(np.append(x,self.bias),self.weights)
        #plug sum into the activation function and return that
        return self.sigmoid(x_sum)

    def set_weights(self, w_init):
        """ w.init is python list with weights """
        self.weights = np.array(w_init)

    def sigmoid(self, x):
        """ Evaluate sigmoid function for the floating point input x """
        return 1/(1+np.exp(-x))

#test code
neuron = Perceptron(inputs=2)
neuron.set_weights([10,10,-15]) #AND [10 for inputs and -15 for bias]


class MultiLayerPerceptron:
    """MLP perception class that uses the Perception class above
    Attributes:
        Layers: A Python list with the number of elements per layer
        bias: The bias term. The same bias is used for all neurons
        eta: The learning rate.
    """
    def __init__(self, layers, bias = 1.0):
        """Return a new MLP object with the specified parameters """
        self.layers = np.array(layers, dtype=object) #layers list to array
        self.bias = bias #bias is member of layer
        self.network = [] #the list of lists of neurons
        self.values = [] #the list of lists of output values

        #Two nested loops to create the neurons layer by layer
        #Each layer needs a list of values and neurons
        
        for i in range(len(self.layers)):
            self.values.append([])
            self.network.append([])
            self.values[i] = [0.0 for j in range(self.layers[i])]
            if i>0: #network[0] is the input layer, so it has no neurons
                for j in range(self.layers[i]):
                    self.network[i].append(Perceptron(inputs=self.layers[i-1], bias=self.bias)) #Perceptron main issue/error
        #Turn new list into numpy arrays
        self.network = np.array([np.array(x) for x in self.network], dtype=object)
        self.values = np.array([np.array(x) for x in self.values], dtype=object)

    #Challenge
    
    def set_weights(self, w_init):
        #Write all weights into neural network
        #w_init is a list of floats. Organize as you'd like
        pass
    
    def printWeights(self):
        print()
        for i in range(1, len(self.network)):
            for j in range(self.layers[i]):
                print("Layer",i+1,"Neuron",j,self.network[i][j].weights)
        print()

    def run(self, x):
        #Run an input forward through neural network
        #x is a python list with input values
        return self.values[-1]
    
    def set_weights(self, w_init):
        """Set the weights
        w_init is a list of lists with the weights for all but the input layer
        """
        for i in range(len(w_init)):
            for j in range (len(w_init[i])):
                self.network[i+1][j].set_weights(w_init[i][j])
    
    def printWeights(self):
        print()
        for i in range(1, len(self.network)):
            for j in range(self.layers[i]):
                print("Layer",i+1,"Neuron",j,self.network[i][j].weights)
        print()
    
    def run(self, x):
        """Feed a sample x into the MLP"""
        x = np.array(x,dtype=object)
        self.values[0] = x
        for i in range(1, len(self.network)):
            for j in range(self.layers[i]):
                self.values[i][j] = self.network[i][j].run(self.values[i-1])
        return self.values[-1]

#test code
mlp = MultiLayerPerceptron(layers=[2,2,1]) #mlp
mlp.set_weights([[[-10,-10,15],[15,15,-10]],[[10,10,-15]]])
mlp.printWeights()
print("MLP:")
print("0 0 = {0:.10f}".format(mlp.run([0,0])[0]))
print("0 1 = {0:.10f}".format(mlp.run([0,1])[0]))
print("1 0 = {0:.10f}".format(mlp.run([1,0])[0]))
print("1 1 = {0:.10f}".format(mlp.run([1,1])[0]))

